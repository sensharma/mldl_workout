h_params:
    # "train_data_limit": None,  # Use None to train on full dataset, 100 etc. for test
    # "val_data_limit": None,    # Use None to train on full dataset, 100 etc. for test
    n_rnn_layers: 4
    random: False
    bi_dir: False
    rnn_dim: 512  # hidden_size param
    n_feats: 80   # input_size param (no. of mel filters)
    dropout: 0.1
    residual: True
    # "optimizer": "adam",
    # [2, 5, 8]: n: predict f_{t + n} | f_{t} (2, 5, 10, 20 used in paper)
    time_shifts: [2, 5, 8]
    clip_threshold: 1.0
    learning_rate: !!float 1e-4  #explicit float; else pyyaml loads this as string
    # (8-12G(gtx 1070, K80, T4): 64; 16G(P100): 128; 16G(V100): 256?)
    batch_size: 32
    # "n_epochs": 2,  # 100 in paper
dataloader_params:
    num_workers: 4 
    pin_memory: True
dataset_params:
    train_url: train-clean-100
    val_url: dev-clean
    test_url: test-clean
    folder: LibriSpeech
